\section{AI Workflows}
\label{sec:ai}

Artificial intelligence (AI) and machine learning (ML) techniques are becoming increasingly popular within the scientific community. Many workflows now integrate ML models to guide analysis, couple simulation and data analysis codes, and exploit specialized computing hardware (e.g., GPUs, neuromorphic chips)~\cite{zhou2017machine}. These workflows inherently couple various types of tasks, such as short ML inference, multi-node simulations, and long-running ML model training. They are also often iterative and dynamic, with learning systems deciding in real time how to modify the workflow, for example, by adding new simulations or changing the workflow all together. AI-enabled workflow systems therefore must be able to optimally place and manage single- and multi-node tasks, exploit heterogeneous architectures (CPUs, GPUs, and accelerators), and seamlessly coordinate the dynamic coupling of disparate simulation tools.


%% Subsection
\subsection{Brief State-of-the-art and Challenges}

Workflows empowered with ML techniques largely differ from traditional workflows running on HPC systems. 
%While workflows (i.e., one large-scale application simulating a scientific object or process) traditionally take little input data and produce large outputs, 
ML workflows that target model training usually require an enormous quantity of input data (either via files or from a collection of databases) and produce a small number of trained models. After training, these models are used to infer new quantities (during ``model inference") and behave like very lightweight applications that produce small output data volumes. These models can be stand-alone applications, or even embedded within larger traditional simulations. There exists an inherent tension between traditional HPC, which evolved around executing large capacity-style codes and AI-HPC, which requires the coordinated execution of many smaller capability-scale applications (e.g., large ensembles of data generation co-mingled with inference and coinciding with periodic retraining of models).

With its reliance of data, effective AI workflows should provide \textbf{\emph{fine-grained data management}} and versioning features, as well as adequate data provenance capabilities. This data management must be flexible: some applications and workflows may move data via a file-system, while others may be better served from a traditional database, data store, or a streaming dataflow model. During inference, it may be best to couple the (lightweight) model as close to the data it is processing as possible. In any case, effective data management is a key feature of successful AI workflows.

%Another key feature of 
AI workflows often require non-traditional hardware, such as GPUs and tensor processing units (TPUs), which can significantly accelerate both training and inference steps. Workflow systems thus need to provide mechanisms for managing execution on \textbf{\emph{heterogeneous resources}}, for example, by offloading heavy computations to GPUs, and managing data between GPU and CPU memory hierarchies. Furthermore, since ML training and inference may be best executed on different hardware from the main simulation, AI workflows may need to enable execution on \textbf{\textit{multi-machine federated systems}} (with the main code executed on a traditional HPC system and the ML model training or inference on a separate system). Further, it is also necessary to provide tight \textbf{\emph{integration with widely-used ML frameworks}}, the development of which is not driven by the HPC community. ML frameworks use Python and R-based libraries and do not follow the classic HPC model: C/C++/Fortran, MPI, and OpenMP, and submission to an HPC batch scheduler. Yet, some efforts in the HPC community seem promising, such as LBANN~\cite{lbann}, EMEWS~\cite{ozik_population_2021}, and eFlows4HPC~\cite{eflows}. Other approaches like Merlin~\cite{merlin} blend HPC and cloud technologies to enable federated workflows.  However, there is a clear disconnect between HPC motivations, needs, and requirements, and current AI practices.

Finally, one of the major differences between traditional and AI workflows is the inherent \textbf{\emph{iterative nature of ML processes}}---AI workflows often feature feedback loops over a data set. Data are created, the model is retrained, and its accuracy evaluated. ML training tasks might leverage hyperparameter optimization frameworks~\cite{akiba2019optuna, bergstra2013hyperopt, wozniak2018} to adjust their execution settings in real time. The final trained model is often used to select new data to acquire (in an ``active learning" environment, the model is used to decide which new simulations to run to better train the model on the next iteration). By design, ML-empowered workflows are dynamic, in contrast to traditional workflows with more structured and deterministic computations. At runtime, the workflow execution graph can potentially evolve based on internal metrics (accuracy), which may reshape the graph or trigger task preemption. Workflow systems should thus support \textbf{\emph{dynamic branching}} (e.g., conditionals, criteria) and partial workflow re-execution on-demand.



%% Subsection
\subsection{A Vision for Potential Community Activities}

To address the disconnect between HPC systems and AI workflows, the community needs to develop sets of example \textbf{\emph{use cases for sample problems}} with representative workflow structures and data types. In addition to expanding upon the above challenges, the community could ``codify'' these challenges in example use cases. However, the set of challenges for enabling AI workflows is extensive. The community thus should define a \textbf{\emph{systematic process}} for identifying and categorizing these challenges. A short-term recommendation would be to write a ``community white paper" about AI Workflow challenges and needs.

Building on the use cases above for the needs and requirements of AI workflows, the community could define \textbf{\emph{AI-Workflow mini-apps}}, which could be used to pair with vendors/future HPC developers so that the systems can be benchmarked against these workflows, and therefore support the co-design of emerging or future systems (e.g., MLCommons~\cite{mlcommons} and the Collective Knowledge framework~\cite{fursin2020collective}).
